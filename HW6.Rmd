---
title: "HW6"
author: "David Lurie"
date: "2024-03-21"
output: pdf_document
---

**2.**

*a)*

```{r}
load("C:/Users/david/Downloads/hw6data.Rdata")
library(devtools)
library(factoextra)
pr.out=prcomp(threesevens,center=T,scale=T)
fviz_pca(pr.out,geom="point",col.ind=as.factor(label))
```

It seems as though the points are well separated along the axis of the second dimension. The first dimension clearly holds a lot of variance, while the second does so as well but also helps to show two clear clumpings of points of different classes.

*b)*

```{r}
library(ggplot2)
library(data.table)

eigs=pr.out$sdev^2
var.df=transpose(as.data.frame(rbind(
  SD = sqrt(eigs),
  Proportion = eigs/sum(eigs),
  Cumulative_Variance = cumsum(eigs)/sum(eigs),
  PC = c(1:256))))

colnames(var.df)=c("SD","Proportion","Cumulative_Variance","PCs")

ggplot(var.df) + geom_line(mapping=aes(x=PCs,y=Cumulative_Variance)) + geom_hline(yintercept=.9) + geom_hline(yintercept=.5)
```

```{r}
nrow(var.df)-length(which(var.df$Cumulative_Variance>=.5))+1
nrow(var.df)-length(which(var.df$Cumulative_Variance>=.9))+1
```

The first 9 PCs explain 50% of the variance; the first 66 explain 90%.

*c)*

```{r}
var_explained = pr.out$sdev^2 / sum(pr.out$sdev^2)

#create scree plot
library(ggplot2)

var.explained=data.frame("Eigenvalue"=eigs[1:20],"PCs"=c(1:20))
ggplot(var.explained) + geom_line(mapping=aes(x=PCs,y=Eigenvalue)) + scale_x_continuous(breaks=seq(0,20,by=1))
```

I would probably choose the first 3 PCs, as this marks a change in steepness of the curve.

**3.**

```{r}
# Compute either K-means clustering
#
# Arguments:
# x: data matrix, n observations (rows) by p features (cols).
# centers: a vector giving the starting centers. Defaults
#   to NULL in which case we choose k centers at random.
# k: number of clusters. If the centers argument is specified,
#   then this doesn't need to be specified.
# maxiter: Maximum number of iterations before we quit. 
#   Defaults to 100.
#
# Returns:
# centers: a matrix of size k x p, giving the final centers.
# cluster: a vector of length n, giving the final clustering
#   assignments.
# iter: number of iterations performed.
# 

euc.dist=function(x1, x2) sqrt(sum((x1 - x2) ^ 2))

library(dplyr)

my.kmeans = function(x, centers=NULL, k=NULL, maxiter=20) {
  n = nrow(x)
  p = ncol(x)
  # Initialize the centers, unless there were supplied in the function  call
  # If centers are not given, k must be specified so we know how many to choose randomly
  if (is.null(centers)) {
    if (is.null(k)) stop("Either centers or k must be specified.")
    centers = matrix(runif(k*p,min(x),max(x)),nrow=k)
  }
  #We can get k from the number of centers we were given
  k = nrow(centers)
  
  cluster = matrix(0,nrow=0,ncol=n)
  cluster.old = cluster
  
  for (iter in 1:maxiter) {
    
    for (i in 1:n){
      nearestCenter=NULL
      nearestCenterDist=10**10
      for (j in 1:nrow(centers)){
        dist=euc.dist(x[i,],centers[j,])
        if (dist<nearestCenterDist){
          nearestCenter=j
          nearestCenterDist=dist
        }
      }
      cluster[i]=nearestCenter
    }
    
    pointClasses=as.data.frame(cbind(x,cluster))
    colnames(pointClasses)=c("V1","V2","Cluster")
    centers = pointClasses %>% group_by(Cluster) %>% summarise(across(everything(),mean))
    centers=as.matrix(centers)[,c(2:3)]
  

    if (iter > 1 & all(cluster == cluster.old)){
      print(iter)
      break
    }
    cluster.old=cluster
  }
  return(list(centers=centers,cluster=cluster,iter=iter))
}
```

```{r}
#Simple test example:
set.seed(0)
x = rbind(matrix(rnorm(2*100,sd=0.2),ncol=2),
          scale(matrix(rnorm(2*100,sd=0.3),ncol=2),cent=-c(1,1),scal=F),
          scale(matrix(rnorm(2*100,sd=0.2),ncol=2),cent=-c(0,1),scal=F))
#Use 3 clusters
k = 3
#Initialize the centers
cent.init = rbind(c(0.5,1),c(1,0),c(0,0.5))

km_yours = my.kmeans(x,centers=cent.init,maxiter=20)
km_truth = kmeans(x,centers=cent.init,iter.max = 20, algorithm = "Lloyd")

xtabs(~km_yours$cluster+km_truth$cluster)

#Plot your success!
nicecolors = c("#E69F00", "#009E73", "#0072B2", "#CC79A7")
plot(x[,1],x[,2],pch=20,col=nicecolors[km_yours$cluster])
points(km_yours$centers,pch=20,cex=3,col=nicecolors)
```



**4**

```{r}
#Clear the workspace
rm(list=ls())

#Load the data.  NAs are coded as "?"
dat = read.csv("C:/Users/david/Downloads/breast-cancer-wisconsin.data",header=FALSE,na.strings="?")
#Label the data fields.  Description and values are as noted.
names(dat) = c(
  'id',#Sample code number            id number
  'thickness',#Clump Thickness               1 - 10
  'size_uniformity',#Uniformity of Cell Size       1 - 10
  'shape_uniformity',#Uniformity of Cell Shape      1 - 10
  'adhesion',#Marginal Adhesion             1 - 10
  'size',#Single Epithelial Cell Size   1 - 10
  'nuclei',#Bare Nuclei                   1 - 10
  'chromatin',#Bland Chromatin               1 - 10
  'nucleoli',#Normal Nucleoli               1 - 10
  'mitoses',#Mitoses                       1 - 10
  'class'#Class:            (2 for benign, 4 for malignant)
  )
#Drop ID number, we don't want to use this for anything
dat = dat[,-1]
#Change outcome to be descriptive
dat$class = ifelse(dat$class==2,"benign","malignant")
#Make outcome a factor
dat$class = as.factor(dat$class)
#Drop a few missing things
dat = dat[complete.cases(dat),]

#Tumor characteristics
X = dat[,1:9]
#Tumor class
y = dat[,10]

### TODO ###
### Cluster the tumors based on X
### Use kmeans function, and be sure to set algorithm="Lloyd"
### The cluster assignments are returned as the $cluster piece of the object kmeans returns
### Compare the cluster assignments to y (using xtabs or table)
```

*a)*

```{r}
kmeans=kmeans(X,centers=2,nstart=10,iter.max=20)
```

```{r}
kmeans.class=ifelse(kmeans$cluster==1,"benign","malignant")
table.class=table(kmeans.class,dat$class)
table.class
(table.class[1,2]+table.class[2,1])/sum(table.class)
```
K-means does seem to "recover" the groups fairly well. It misclassifies 3.95% of observations with k=2. 

*b)*

Repeating the process with the opposite classification from the k-means algorithm would generate a misclassification rate of 1-.0395=.9605. From this, it is clear that it makes sense to classify the first class (class 1) as "benign" and class 2 as "malignant".
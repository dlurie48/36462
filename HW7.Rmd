---
title: "HW7"
author: "David Lurie"
date: "2024-03-26"
output: pdf_document
---

**1.**

*a)*

```{r}
load("C:/Users/david/Downloads/hw7hierdata.RData")
```

```{r}
library(tidyverse)
ggplot(as.data.frame(hdata)) + geom_point(mapping=aes(x=V1,y=V2))
```

```{r}
clust=hclust(dist(hdata),method="average")
plot(clust)
```

*b)*

We can clearly see the two main clusters indicated by the first line. The large group to the left in the deprogram contains four subgroups, while the group on the right contains 3. The subtrees of the 7 subgroups are indicated by the lower horizontal line cutting the tree into 7 subtrees.

*c)*

```{r}
k=2
clust.2=cutree(clust,k=k)
par(mfrow=c(1,2))
plot(hdata,col=clust.2)
plot(clust)
abline(h=mean(rev(clust$height)[(k-1):k]))
```

Clustering does clearly identify the two large groups.

```{r}
k=4
clust.2=cutree(clust,k=k)
par(mfrow=c(1,2))
plot(hdata,col=clust.2)
plot(clust)
abline(h=mean(rev(clust$height)[(k-1):k]))
```

Here the clustering identifies the second of the two large groups and the three subgroups of the first of the two large groups.

```{r}
k=7
clust.2=cutree(clust,k=k)
par(mfrow=c(1,2))
plot(hdata,col=clust.2)
plot(clust)
abline(h=mean(rev(clust$height)[(k-1):k]))
```

Here clustering successfully identifies the 7 subgroups of the 2 large groups.

In each case this does match the labeling of trees from looking at the dendrogram.

*d)*

```{r}
clust.single=hclust(dist(hdata),method="single")
plot(clust.single)
```

The groups and subgroups are definitely not as clear from the dendrogram as before.

```{r}
k=2
clust.2=cutree(clust.single,k=k)
par(mfrow=c(1,2))
plot(hdata,col=clust.2)
plot(clust)
abline(h=mean(rev(clust.single$height)[(k-1):k]))
```

```{r}
k=7
clust.2=cutree(clust.single,k=k)
par(mfrow=c(1,2))
plot(hdata,col=clust.2)
plot(clust)
abline(h=mean(rev(clust.single$height)[(k-1):k]))
```

Single linkage results in linking, where points that are clearly part of one group or subgroup but happen to be farther from another point in that subgroup than another point that connects that subgroup to a different group end up clustered into a different group. So the clusters recovered here by single linkage don't at all recover the original groups and don't seem like reasonable clusters.

**2.**

*a)*

```{r}
library(mixtools)
library(MASS)

set.seed(1)
n = 1300;
Sigma1 = matrix(c(1,0.5,0.5,1),nrow=2)
Sigma2 = matrix(c(1,-0.9,-0.9,1),nrow=2)
Sigma3 = Sigma1
x1 = mvrnorm(500, mu=c(0,0), Sigma=Sigma1)
x2 = mvrnorm(500, mu=c(4,1), Sigma=Sigma2)  
x3 = mvrnorm(300, mu=c(8,1), Sigma=Sigma3)
y1 = matrix(1,500,1)
y2 = matrix(2,500,1)
y3 = matrix(3,300,1) 
x = rbind(x1,x2,x3)
y = rbind(y1,y2,y3)

plot(x,pch=19,col=y)
```

```{r}
em=mvnormalmixEM(x,k=3)
```
*a)*

```{r}
gmm.class=apply(em$posterior,1,which.max)
plot(x,col=gmm.class)
legend("topleft", legend=levels(as.factor(gmm.class)), pch=16, col=unique(gmm.class))
```

*b)*

```{r}
gmm.unsure=apply(em$posterior,1,function(row) all(row < 0.75))
gmm.class[gmm.unsure]=4
plot(x,col=gmm.class)
legend("topleft", legend=c(1,2,"unsure",3), pch=16, col=unique(gmm.class))
```

**3.**

*a)*

```{r}
set.seed(0)
n = 200 # each cluster has n points, total data will have 2*n points

# The following generates the data for the inner circle
t1 = runif(n)
X1 = cbind(sin(2*pi*t1), cos(2*pi*t1)) + matrix(rnorm(2*n),ncol=2)*0.1

# This is the outer circle
t2 = runif(n)
X2 = 3*cbind(sin(2*pi*t2), cos(2*pi*t2)) + matrix(rnorm(2*n),ncol=2)*0.1

# stack the two matrices together, forming a 400 by 2 matrix, where the top 200 rows correspond to the inner circle while the bottom 200 rows correspond to the outer circle
X = rbind(X1, X2)
plot(X, col=c(rep("red", n), rep("blue", n)))

# You will need the package "rdist" here.
library(rdist)
dist.matrix = as.matrix(rdist(X)) # This is the pairwise Euclidean distance

# TODO: Here is your choice of sigma
sigma = .1
# try different values

# TODO: Weight matrix using the Gaussian kernel and the sigma chosen above
# Remember to remove the diagonal!  Tip: diag(2*n) gives you the diagonal matrix of size 2*n with diagonals being 1.
num_points <- nrow(X)

# Initialize the weighted adjacency matrix W

W <- matrix(0, nrow = num_points, ncol = num_points)

# Compute the weights
for (i in 1:num_points) {
  for (j in 1:num_points) {
    if (i != j) {
      W[i, j] <- exp(-sum((X[i,] - X[j,])^2) / (sigma^2))
    }
  }
}

W <- (W + t(W)) / 2

# Set diagonal elements to 0
diag(W) <- 0

# Fill in this part 

# Degree matrix
D = diag(apply(W, 2, sum))

# Graph Laplacian
L = D - W

# The second smallest eigenvector
v2 = eigen(L)$vectors[,2*n-1]

# TODO: check out this plot for different values of sigma. Does v2 reveal the clusters? Do you really need to threshold v2 in order to obtain the clustering?
plot(v2)


# TODO: Look at the between-cluster and within-cluster edge weights, and gain some intuition when a tuning parameter sigma captures the geometry of the data.  Make sure you try different values of sigma 
par(mfrow=c(1,3)) 

# (i) edges with one point in the inner circle, and the other one in the outer circle
boxplot(as.vector(W[1:n,(n+1):(2*n)]),ylim=c(0,1))

# (ii) edges with both points in the inner circle
boxplot(W[1:n,1:n][upper.tri(diag(n))],ylim=c(0,1))

# (iii) edges with both points in the outer circle
boxplot(W[(n+1):(2*n),(n+1):(2*n)][upper.tri(diag(n))],ylim=c(0,1))
```

*b)*

```{r}
computeW=function(X,sigma){
  num_points <- nrow(X)
  W <- matrix(0, nrow = num_points, ncol = num_points)
  for (i in 1:num_points) {
  for (j in 1:num_points) {
    if (i != j) {
      W[i, j] <- exp(-sum((X[i,] - X[j,])^2) / (sigma^2))
    }
  }
}
  W <- (W + t(W)) / 2
  diag(W) <- 0
  D = diag(apply(W, 2, sum))
  L = D - W
  v2 = eigen(L)$vectors[,2*n-1]
  plot(v2,main=sigma)
}
for (i in c(.001,5)){
  computeW(X,i)
}
```

For a sigma value of .001, the v2 plot shows the same value for all but the first index, so we can clearly not see the separated clusters well. For a sigma of .1 in part a), we can see that the two groups below and above index of 200 are separated into two values of v2. Finally, for a sigma of 10 the separation between clusters is visible in the plot but not clearly by a value of v2, rather by a different variance in the two groups.
